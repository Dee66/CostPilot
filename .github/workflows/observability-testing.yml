# MANUAL WORKFLOW â€” intentionally not automated. Run only when explicitly required.
name: Observability Testing

on:
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of observability test to run'
        required: false
        default: 'quick'
        type: choice
        options:
          - quick
          - logging
          - monitoring
          - tracing
          - dashboard
          - alerting
          - full

jobs:
  quick-observability-test:
    name: Quick Observability Test
    runs-on: ubuntu-latest
    if: github.event_name != 'schedule' || github.event.inputs.test_type == 'quick'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy

    - name: Cache dependencies
      uses: Swatinem/rust-cache@v2

    - name: Build CostPilot
      run: |
        cd products/costpilot
        cargo build --release --features observability

    - name: Run quick observability tests
      run: |
        cd products/costpilot
        ../../scripts/run_observability_testing.sh logging

    - name: Upload observability results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: quick-observability-results-${{ github.run_number }}
        path: |
          products/costpilot/observability-results/
        retention-days: 7

  logging-validation:
    name: Logging Validation
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'logging'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Cache dependencies
      uses: Swatinem/rust-cache@v2

    - name: Build CostPilot
      run: |
        cd products/costpilot
        cargo build --release --features observability

    - name: Run logging validation tests
      run: |
        cd products/costpilot
        ../../scripts/run_observability_testing.sh logging
      timeout-minutes: 30

    - name: Upload logging results
      uses: actions/upload-artifact@v4
      with:
        name: logging-validation-results-${{ github.run_number }}
        path: |
          products/costpilot/observability-results/logging/
        retention-days: 30

  monitoring-accuracy:
    name: Monitoring Accuracy Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'monitoring'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Cache dependencies
      uses: Swatinem/rust-cache@v2

    - name: Build CostPilot
      run: |
        cd products/costpilot
        cargo build --release --features observability

    - name: Run monitoring accuracy tests
      run: |
        cd products/costpilot
        ../../scripts/run_observability_testing.sh monitoring
      timeout-minutes: 45

    - name: Upload monitoring results
      uses: actions/upload-artifact@v4
      with:
        name: monitoring-accuracy-results-${{ github.run_number }}
        path: |
          products/costpilot/observability-results/monitoring/
        retention-days: 30

  tracing-implementation:
    name: Tracing Implementation Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'tracing'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Cache dependencies
      uses: Swatinem/rust-cache@v2

    - name: Build CostPilot
      run: |
        cd products/costpilot
        cargo build --release --features observability

    - name: Run tracing implementation tests
      run: |
        cd products/costpilot
        ../../scripts/run_observability_testing.sh tracing
      timeout-minutes: 30

    - name: Upload tracing results
      uses: actions/upload-artifact@v4
      with:
        name: tracing-implementation-results-${{ github.run_number }}
        path: |
          products/costpilot/observability-results/tracing/
        retention-days: 30

  dashboard-functionality:
    name: Dashboard Functionality Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'dashboard'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Cache dependencies
      uses: Swatinem/rust-cache@v2

    - name: Build CostPilot
      run: |
        cd products/costpilot
        cargo build --release --features observability

    - name: Run dashboard functionality tests
      run: |
        cd products/costpilot
        ../../scripts/run_observability_testing.sh dashboard
      timeout-minutes: 20

    - name: Upload dashboard results
      uses: actions/upload-artifact@v4
      with:
        name: dashboard-functionality-results-${{ github.run_number }}
        path: |
          products/costpilot/observability-results/dashboard/
        retention-days: 30

  alerting-effectiveness:
    name: Alerting Effectiveness Validation
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'alerting'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Cache dependencies
      uses: Swatinem/rust-cache@v2

    - name: Build CostPilot
      run: |
        cd products/costpilot
        cargo build --release --features observability

    - name: Run alerting effectiveness tests
      run: |
        cd products/costpilot
        ../../scripts/run_observability_testing.sh alerting
      timeout-minutes: 25

    - name: Upload alerting results
      uses: actions/upload-artifact@v4
      with:
        name: alerting-effectiveness-results-${{ github.run_number }}
        path: |
          products/costpilot/observability-results/alerting/
        retention-days: 30

  observability-regression-check:
    name: Observability Regression Check
    runs-on: ubuntu-latest
    if: always()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download observability results
      uses: actions/download-artifact@v4
      with:
        name: quick-observability-results-${{ github.run_number }}
        path: observability-results/

    - name: Check for observability regressions
      run: |
        # Check logging validation results
        if [ -f "observability-results/logging/log-structure-validation.json" ]; then
          STRUCTURE_SCORE=$(jq '.validation_score' observability-results/logging/log-structure-validation.json 2>/dev/null || echo "0")
          if (( $(echo "$STRUCTURE_SCORE < 95" | bc -l) )); then
            echo "ðŸš¨ Logging structure regression detected: ${STRUCTURE_SCORE}% compliance"
            echo "observability_regression=true" >> $GITHUB_OUTPUT
          fi
        fi

        # Check monitoring accuracy
        if [ -f "observability-results/monitoring/monitoring-accuracy-analysis.json" ]; then
          MONITORING_ACCURACY=$(jq '.overall_monitoring_effectiveness' observability-results/monitoring/monitoring-accuracy-analysis.json 2>/dev/null || echo "false")
          if [[ "$MONITORING_ACCURACY" != "true" ]]; then
            echo "ðŸš¨ Monitoring accuracy regression detected"
            echo "observability_regression=true" >> $GITHUB_OUTPUT
          fi
        fi

        # Check alerting effectiveness
        if [ -f "observability-results/alerting/alerting-analysis.json" ]; then
          ALERTING_EFFECTIVE=$(jq '.alerting_comprehensive' observability-results/alerting/alerting-analysis.json 2>/dev/null || echo "false")
          if [[ "$ALERTING_EFFECTIVE" != "true" ]]; then
            echo "ðŸš¨ Alerting effectiveness regression detected"
            echo "observability_regression=true" >> $GITHUB_OUTPUT
          fi
        fi

        # Default to no regression if no results found
        if [[ "${{ steps.regression_check.outputs.observability_regression }}" != "true" ]]; then
          echo "âœ… No observability regressions detected"
          echo "observability_regression=false" >> $GITHUB_OUTPUT
        fi
      id: regression_check

  observability-summary:
    name: Observability Test Summary
    runs-on: ubuntu-latest
    if: always()
    needs: [quick-observability-test, logging-validation, monitoring-accuracy, tracing-implementation, dashboard-functionality, alerting-effectiveness]

    steps:
    - name: Download all observability results
      uses: actions/download-artifact@v4
      with:
        path: all-observability-results/

    - name: Generate comprehensive observability report
      run: |
        echo "## ðŸ” Observability Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Logging validation summary
        if [ -f "all-observability-results/logging-validation-results-*/log-structure-validation.json" ]; then
          STRUCTURE_SCORE=$(find all-observability-results/ -name "log-structure-validation.json" -exec jq '.validation_score' {} \; 2>/dev/null | head -1 || echo "0")
          AUDIT_INTEGRITY=$(find all-observability-results/ -name "audit-trail-validation.json" -exec jq '.audit_trail_integrity' {} \; 2>/dev/null | head -1 || echo "false")

          echo "- **Logging Structure Compliance**: ${STRUCTURE_SCORE}%" >> $GITHUB_STEP_SUMMARY
          echo "- **Audit Trail Integrity**: $AUDIT_INTEGRITY" >> $GITHUB_STEP_SUMMARY
        fi

        # Monitoring accuracy summary
        if [ -f "all-observability-results/monitoring-accuracy-results-*/monitoring-accuracy-analysis.json" ]; then
          MONITORING_EFFECTIVE=$(find all-observability-results/ -name "monitoring-accuracy-analysis.json" -exec jq '.overall_monitoring_effectiveness' {} \; 2>/dev/null | head -1 || echo "false")

          echo "- **Monitoring Effectiveness**: $MONITORING_EFFECTIVE" >> $GITHUB_STEP_SUMMARY
        fi

        # Tracing implementation summary
        if [ -f "all-observability-results/tracing-implementation-results-*/tracing-analysis.json" ]; then
          TRACING_COMPREHENSIVE=$(find all-observability-results/ -name "tracing-analysis.json" -exec jq '.tracing_comprehensive' {} \; 2>/dev/null | head -1 || echo "false")
          BOTTLENECK_DETECTION=$(find all-observability-results/ -name "tracing-analysis.json" -exec jq '.bottleneck_detection' {} \; 2>/dev/null | head -1 || echo "false")

          echo "- **Distributed Tracing**: $TRACING_COMPREHENSIVE" >> $GITHUB_STEP_SUMMARY
          echo "- **Bottleneck Detection**: $BOTTLENECK_DETECTION" >> $GITHUB_STEP_SUMMARY
        fi

        # Dashboard functionality summary
        if [ -f "all-observability-results/dashboard-functionality-results-*/dashboard-analysis.json" ]; then
          DASHBOARD_COMPREHENSIVE=$(find all-observability-results/ -name "dashboard-analysis.json" -exec jq '.dashboard_comprehensive' {} \; 2>/dev/null | head -1 || echo "false")

          echo "- **Dashboard Functionality**: $DASHBOARD_COMPREHENSIVE" >> $GITHUB_STEP_SUMMARY
        fi

        # Alerting effectiveness summary
        if [ -f "all-observability-results/alerting-effectiveness-results-*/alerting-analysis.json" ]; then
          ALERTING_COMPREHENSIVE=$(find all-observability-results/ -name "alerting-analysis.json" -exec jq '.alerting_comprehensive' {} \; 2>/dev/null | head -1 || echo "false")

          echo "- **Alerting Effectiveness**: $ALERTING_COMPREHENSIVE" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“Š Overall Observability Status" >> $GITHUB_STEP_SUMMARY

        # Calculate overall status
        OVERALL_STATUS="âœ… All observability tests passed"

        # Check for any failures
        if find all-observability-results/ -name "*.json" -exec grep -l '"false"' {} \; | grep -q .; then
          OVERALL_STATUS="âš ï¸ Some observability tests failed - review results"
        fi

        echo "$OVERALL_STATUS" >> $GITHUB_STEP_SUMMARY

    - name: Fail on observability regression
      if: steps.regression_check.outputs.observability_regression == 'true'
      run: |
        echo "Observability regression detected - failing build"
        exit 1
